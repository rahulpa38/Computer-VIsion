{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "test_sentence_tokens = ['a','fact','about','the','unicorn','is','the','same','as','an','alternative','fact','about','the','unicorn','.']\n",
    "\n",
    "words = brown.words()\n",
    "fdist1 = nltk.FreqDist(w.lower() for w in words)\n",
    "\n",
    "total_words = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 69971, ',': 58334, '.': 49346, 'of': 36412, 'and': 28853, 'to': 26158, 'a': 23195, 'in': 21337, 'that': 10594, 'is': 10109, ...})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of tokens in sample sententence in Brown according to NLTK:\n",
      "a 23195\n",
      "fact 447\n",
      "about 1815\n",
      "the 69971\n",
      "unicorn 0\n",
      "is 10109\n",
      "the 69971\n",
      "same 686\n",
      "as 7253\n",
      "an 3740\n",
      "alternative 34\n",
      "fact 447\n",
      "about 1815\n",
      "the 69971\n",
      "unicorn 0\n",
      ". 49346\n"
     ]
    }
   ],
   "source": [
    "print('Frequency of tokens in sample sententence in Brown according to NLTK:')\n",
    "\n",
    "for word in test_sentence_tokens:\n",
    "    print(word,fdist1[word])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given that there are 1161192 in the Brown Corpus, the unigram probability of these words\n",
      "is as follows (rounded to 3 significant digits):\n",
      "a 0.02\n",
      "fact 0.000385\n",
      "about 0.00156\n",
      "the 0.0603\n",
      "unicorn 0.0\n",
      "is 0.00871\n",
      "the 0.0603\n",
      "same 0.000591\n",
      "as 0.00625\n",
      "an 0.00322\n",
      "alternative 2.93e-05\n",
      "fact 0.000385\n",
      "about 0.00156\n",
      "the 0.0603\n",
      "unicorn 0.0\n",
      ". 0.0425\n"
     ]
    }
   ],
   "source": [
    "# input('Pausing: Hit Return when Ready.')\n",
    "\n",
    "print('Given that there are',total_words,'in the Brown Corpus, the unigram probability of these words')\n",
    "print('is as follows (rounded to 3 significant digits):')\n",
    "\n",
    "for word in test_sentence_tokens:\n",
    "    unigram_probability = fdist1[word]/total_words\n",
    "    print(word,float('%.3g' % unigram_probability))\n",
    "    ## print(word,round((fdist1[word]/total_words),3))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words2 = []\n",
    "previous = 'EMPTY'\n",
    "sentences = 0\n",
    "for word in words:\n",
    "    if previous in ['EMPTY','.','?','!']:\n",
    "        ## insert word_boundaries at beginning of Brown,\n",
    "        ## and after end-of-sentence markers (overgenerate due to abbreviations, etc.)\n",
    "        words2.append('*start_end*')\n",
    "    if fdist1[word]==1:\n",
    "        ## words occurring only once are treated as Out of Vocabulary Words\n",
    "        words2.append('*oov*')\n",
    "    else:\n",
    "        words2.append(word)\n",
    "    previous = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words2.append('*start_end*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist2 = nltk.FreqDist(w.lower() for w in words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15673 instances of OOVs\n",
      "Unigram probabilities including OOV probabilities.\n"
     ]
    }
   ],
   "source": [
    "print('There are',fdist2['*oov*'],'instances of OOVs')\n",
    "\n",
    "print('Unigram probabilities including OOV probabilities.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigram_probability(word):\n",
    "    if word in fdist1:\n",
    "        unigram_probability = fdist2[word]/total_words\n",
    "    else:\n",
    "        unigram_probability = fdist2['*oov*']/total_words\n",
    "    return(unigram_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 0.02\n",
      "fact 0.000385\n",
      "about 0.00156\n",
      "the 0.0603\n",
      "unicorn 0.0135\n",
      "is 0.00871\n",
      "the 0.0603\n",
      "same 0.000591\n",
      "as 0.00625\n",
      "an 0.00322\n",
      "alternative 2.93e-05\n",
      "fact 0.000385\n",
      "about 0.00156\n",
      "the 0.0603\n",
      "unicorn 0.0135\n",
      ". 0.0425\n",
      "Calculating bigram counts for sentence, including bigrams with sentence boundaries, i.e., *BEGIN* and *END*\n",
      "Assuming some idealizations: all periods, questions and exclamation marks end sentences;\n"
     ]
    }
   ],
   "source": [
    "for word in test_sentence_tokens:\n",
    "    unigram_probability = get_unigram_probability(word)\n",
    "    print(word,float('%.3g' % unigram_probability))\n",
    "\n",
    "# input('Pausing: Hit Return when Ready.')\n",
    "## make new version that models Out of Vocabulary (OOV) words\n",
    "\n",
    "print('Calculating bigram counts for sentence, including bigrams with sentence boundaries, i.e., *BEGIN* and *END*')\n",
    "print('Assuming some idealizations: all periods, questions and exclamation marks end sentences;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating bigram counts for sentence, including bigrams with sentence boundaries, i.e., *BEGIN* and *END*\n",
      "Assuming some idealizations: all periods, questions and exclamation marks end sentences;\n"
     ]
    }
   ],
   "source": [
    "print('Calculating bigram counts for sentence, including bigrams with sentence boundaries, i.e., *BEGIN* and *END*')\n",
    "print('Assuming some idealizations: all periods, questions and exclamation marks end sentences;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object bigrams at 0x000001E6EE108B10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating bigram counts for sentence, including bigrams with sentence boundaries, i.e., *BEGIN* and *END*\n",
      "Assuming some idealizations: all periods, questions and exclamation marks end sentences;\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ConditionalFreqDist with 34144 conditions>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Calculating bigram counts for sentence, including bigrams with sentence boundaries, i.e., *BEGIN* and *END*')\n",
    "print('Assuming some idealizations: all periods, questions and exclamation marks end sentences;')\n",
    "\n",
    "bigrams = nltk.bigrams(w.lower() for w in words2)\n",
    "## get bigrams for words2 (words plus OOV)\n",
    "# print(*map(' '.join, bigrams), sep=', ')\n",
    "cfd = nltk.ConditionalFreqDist(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token1 in cfd:\n",
    "    if not '*oov*' in cfd[token1]:\n",
    "        cfd[token1]['*oov*']=1\n",
    "    for a in cfd[token1]:\n",
    "        print(a)\n",
    "    print(\"Iteration Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_list(inlist):\n",
    "    out = 1\n",
    "    for number in inlist:\n",
    "        out *= number\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_probability(first,second):\n",
    "    if not second in cfd[first]:\n",
    "        print('Backing Off to Unigram Probability for',second)\n",
    "        unigram_probability = get_unigram_probability(second)\n",
    "        return(unigram_probability)\n",
    "    else:\n",
    "        bigram_frequency = cfd[first][second]\n",
    "    unigram_frequency = fdist2[first]\n",
    "    bigram_probability = bigram_frequency/unigram_frequency\n",
    "    return(bigram_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bigram_freq_of_sentence_token_list(tokens):\n",
    "    prob_list = []\n",
    "    ## assume that 'START' precedes the first token\n",
    "    previous = '*start_end*'\n",
    "    for token in tokens:\n",
    "        if not token in fdist2:\n",
    "            token = '*oov*'\n",
    "        next_probability = get_bigram_probability(previous,token)\n",
    "        print(previous,token,(float('%.3g' % next_probability)))\n",
    "        prob_list.append(next_probability)\n",
    "        previous = token\n",
    "    ## assume that 'END' follows the last token\n",
    "    next_probability = get_bigram_probability(previous,'*start_end*')\n",
    "    print(previous,'*start_end*',next_probability)\n",
    "    prob_list.append(next_probability)\n",
    "    probability = multiply_list(prob_list)\n",
    "    print('Total Probability',float('%.3g' % probability))\n",
    "    return(probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*start_end* a 0.0182\n",
      "a fact 0.000388\n",
      "fact about 0.00447\n",
      "about the 0.182\n",
      "the *oov* 0.0293\n",
      "*oov* is 0.00485\n",
      "is the 0.0786\n",
      "the same 0.00898\n",
      "same as 0.035\n",
      "as an 0.029\n",
      "an alternative 0.00241\n",
      "Backing Off to Unigram Probability for fact\n",
      "alternative fact 0.000385\n",
      "fact about 0.00447\n",
      "about the 0.182\n",
      "the *oov* 0.0293\n",
      "*oov* . 0.0865\n",
      ". *start_end* 1.0\n",
      "Total Probability 1.12e-30\n"
     ]
    }
   ],
   "source": [
    "result = calculate_bigram_freq_of_sentence_token_list(test_sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
